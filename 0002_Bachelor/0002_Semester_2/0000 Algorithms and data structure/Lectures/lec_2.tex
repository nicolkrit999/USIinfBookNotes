\lecture{2}{21 Feb}{27-54}

\section{Algorithms}
\subsection{Data structures}

\begin{definition}[Data structure]\label{def:data_structure_1}
    A way to store and organize data in order to facilitate access and modifications.
\end{definition}

\begin{definition}[Efficiency]\label{def:efficiency_1}
    Different algorithms solve the same problem but have different level of efficiency.
    
\end{definition}

\begin{eg}[Insertion sort vs merge sort]\label{eg:insertion_sort_vs_merge_sort_1}
  We note that the two sorting algorithms have different efficiency
  \begin{note}[Insertion sort]\label{note:insertion_sort_1}
    $C_1*n^2$ where $C_1$ is constant independent of n
  \end{note}

  \begin{note}[Merge sort]\label{note:merge_sort_1}
    $C_2 * n * \log_{2}n$ where $C_2$ is constant independent of n
  \end{note}

  \begin{remark}[Constant factor]\label{def:constant_factor_1}
    insertion sort has a smaller constant factor than merge sort $C_1 < C_2$. The majority of the times constant factor has less influence than input size $n$
  \end{remark}

  \begin{definition}[Constant factor]\label{def:constant_factor_2}
      Anything that doesn't depend on the input parameter(s)
  \end{definition}

  \begin{definition}[Input size]\label{def:constant_factor_2}
      The input size can be the following:

      \begin{itemize}
          \item Number of items in the input, eg the number of items to sort
          \item Total number of bits, eg bitwise multiplication to multiply 2 integers
          \item Input size in term of 2 numbers, eg for finding the shortest path in a graph from a given source, usually the number of vertices plus the number of edges.
      \end{itemize}
  \end{definition}

  \end{eg}

  \chapter{Getting started (39-49)}

  \section{Insertion sort}
  \begin{definition}[keys]\label{def:keys_1}
      The numbers to be sorted. The input comes in the form of an array with $"n"$ elements. The keys are often associated with other data, called "satellite data". Together they form a "record" 
  \end{definition}

  \begin{definition}[Arrays]\label{def:arrays_1}
      A data structure. A collection of items stored at contiguous memory locations. The idea is to store multiple items of the same type together. This makes it easier to calculate the position of each element by simply adding an offset to a base value, i.e., the memory location of the first element of the array (generally denoted by the name of the array).
  \end{definition}

\begin{eg}[How it works]
To sort an array of size N in ascending order iterate over the array and compare the current element (key) \hyperref[def:keys_1]{keys} to its predecessor, if the key element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element.
\end{eg}

\begin{algorithm}
\caption{Insertion sort}\label{alg:insertion_sort_1}
  \For{$i\gets2$ \KwTo $n$}{
  $key = A[1]$ \tcp*{Assign variable "key" to the index "i" of array "A"}
  
  $j = i - 1$ \tcp*{ Insert $A[1]$ into the sorted sub-array $A[1 : i -1]$}
  
  \tcc{Conditional check while true}
  \While{$j > 0$ and $A[j] > key$}{
  $A[j + 1] = A[j]$ \tcp*{Retrive value at index "j" and assign this value to the element at index $j + 1$}
  
  $j = j - 1$} \tcp*{Swap $j$ with the value at index $j - 1$}
  $A]j + 1] = key$
  }

\end{algorithm}



\subsubsection{Loop invariant}

\begin{definition*}
Loop invariant
\begin{definition}[Loop invariant]\label{def:loop_invariants_1}
    A loop invariant is a condition (among program variables) that is necessarily true immediately before and immediately after each iteration of a loop. 
\end{definition}
\begin{note}[Boolean state thorugh the iteration]\label{note:loop_invariant_iteration_boolean_state_1}
    Note that this says nothing about its truth or falsity part way through an iteration.
\end{note}

\end{definition*}


\section{Analyzing algorithms}

\begin{definition}[Running time]\label{def:running_time_1}
    Number of instruction and data accesses executed
\end{definition}

\begin{note}[running time]\label{note:running_time_1}
The running time is dependent on various factor, such as \hyperref[def:constant_factor_2]{input size}, current status\footnote{How many keys are already sorted}, machine hardware.

However you can evaluate a formula depending on the input size, which is a general explanation of the time complexity

\end{note}


\subsubsection{Insertion sort execution cost evaluation method 1: Time cost of each statement}

\begin{note}[running time evaluation]\label{note:statement_execution_number_1}
    The running time evaluation is the sum of the products of the "cost", which is 1 for each statement and "times", which is dependent on "n".
    
    \begin{remark}[For and while loop]
         In "For" and "while" loop the first statement, called \emph{test} is executed one more time than the loop body.
        This is because the loop stop after the test result in being false, which require an additional step.\footnote{to actually check if it "false"}
    \end{remark}
\end{note}

\begin{eg}[for i = m to n]
    Where "i" is the index counter starting from "m" and "n" is the number of keys in the Array "A".   
    
    It has a cost of c1, since it's the first statement.
    
    The execution time is "n", to show that it scales linearly with n
\end{eg}


\begin{eg}[key = A{[1]}]
    Assign a variable named "key" to the index "i" in the Array "A"
    
    It has a cost of c2, since it's the second statement and it is executed n times, to show that it scales linearly with n

    The execution time is "n - 1" because it consider all the n iteration, one for each key, except the first one (2 in this case).

    \begin{remark}[n - 1]
        The same applies for the entire body of the for loop, including the body of the while loop.\footnote{The test is executed one more time than the body}
    \end{remark}
\end{eg}


\begin{eg}[while j > 0 and A{[j]} > key]
    Assign a variable named "key" to the index "i" in the Array "A"
    
    It has a cost of c5, since it's the fifth statement and it is executed n times, to show that it scales linearly with n

    The execution time is $\sum_{i}^{n}2^{t_1}$ because it consider all the n iteration, one for each key, except the first one (2 in this case).

    \begin{remark}[T(n)]
        "T" is only a letter to denote "time of \emph{n}" and not a value in itself.

        It express the concept of \hyperref[def:running_time_1]{running time}
    \end{remark}
\end{eg}